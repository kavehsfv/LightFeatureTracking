{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we are on colab: this clones the repo and installs the dependencies\n",
    "import torch\n",
    "from lightglue import LightGlue, SuperPoint, DISK\n",
    "from lightglue.utils import load_image_crop, rbd, load_image, read_image\n",
    "from lightglue import viz2d\n",
    "from FNCs import get_key_by_value, replace_key, tensor_to_tuple, frame_exists, numpy_image_to_torch, load_data_image_crop\n",
    "from AlgoFCN import FeatureTracker\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import copy\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import colorsys\n",
    "\n",
    "from IPython.display import display, Image\n",
    "from IPython.display import display, clear_output\n",
    "from IPython.display import Video\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaSOTHorseDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images and annotations.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.img_dir = os.path.join(root_dir, 'img')\n",
    "        self.bbox_file = os.path.join(root_dir, 'groundtruth.txt')\n",
    "        self.occlusion_file = os.path.join(root_dir, 'full_occlusion.txt')\n",
    "        self.out_of_view_file = os.path.join(root_dir, 'out_of_view.txt')\n",
    "\n",
    "        # Load annotations\n",
    "        self.bboxes = pd.read_csv(self.bbox_file, header=None)\n",
    "        self.occlusions = pd.read_csv(self.occlusion_file, header=None).iloc[0]\n",
    "        self.out_of_views = pd.read_csv(self.out_of_view_file, header=None).iloc[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bboxes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, f'{idx+1:08d}.jpg')\n",
    "        bbox = self.bboxes.iloc[idx].values\n",
    "        occlusion = self.occlusions[idx]\n",
    "        out_of_view = self.out_of_views[idx]\n",
    "\n",
    "        sample = {'image_path': img_path, 'bbox': bbox, 'occlusion': occlusion, 'out_of_view': out_of_view}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "# Example transform that converts images to tensor\n",
    "class ToTensorTransform:\n",
    "    def __call__(self, sample):\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n",
      "Current device: Quadro RTX 8000\n",
      "Feature Extractor is:  aliked\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LaSOTHorseDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m tracker \u001b[38;5;241m=\u001b[39m FeatureTracker()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Example of using the dataset with DataLoader\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m horse_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mLaSOTHorseDataset\u001b[49m(root_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/usr/mvl2/ksgh2/Documents/projects/VOT/Datasets/data/lasot/horse/horse-15\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m                                    transform\u001b[38;5;241m=\u001b[39mToTensorTransform())\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# dataloader = DataLoader(horse_dataset, batch_size=1, shuffle= False)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(horse_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LaSOTHorseDataset' is not defined"
     ]
    }
   ],
   "source": [
    "tracker = FeatureTracker()\n",
    "# Example of using the dataset with DataLoader\n",
    "horse_dataset = LaSOTHorseDataset(root_dir='/usr/mvl2/ksgh2/Documents/projects/VOT/Datasets/data/lasot/horse/horse-15',\n",
    "                                   transform=ToTensorTransform())\n",
    "# dataloader = DataLoader(horse_dataset, batch_size=1, shuffle= False)\n",
    "dataloader = DataLoader(horse_dataset, batch_size=1, shuffle=False)\n",
    "print(len(dataloader))\n",
    "\n",
    "\n",
    "plottingFrames = []\n",
    "\n",
    "for i, item in enumerate(dataloader):\n",
    "    imagePath = item['image_path'][0]\n",
    "    bbox = item['bbox'][0]\n",
    "    occlusion = item['occlusion'][0]\n",
    "    outOfView = item['out_of_view'][0]\n",
    "\n",
    "    desRec = [bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]]\n",
    "    desRec = [x.item() for x in desRec]\n",
    "    isOnline = False\n",
    "    __cpTracks, __kpMvmt, image_cv2 = tracker.process_frame(imagePath, i, desRec, isOnline)\n",
    "    cvFrame = tracker.update_cvFrame(__cpTracks, image_cv2, i, desRec, _isOnline = False, pltTrackLines = True)\n",
    "\n",
    "    plottingFrames.append(cvFrame)\n",
    "\n",
    "    # if (not occlusion) or (not outOfView): \n",
    "    #     pass\n",
    "\n",
    "    # print(f\"Item {i+1}\")\n",
    "    # print(imagePath, bbox, occlusion, outOfView)\n",
    "\n",
    "    # image = read_image(imagePath).permute(1, 2, 0) / 255  # Convert the image to the [0, 1] range\n",
    "\n",
    "    # fig, ax = plt.subplots()\n",
    "    # ax.imshow(image)\n",
    "    # # rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2] - bbox[0], bbox[3] - bbox[1], linewidth=1, edgecolor='r', facecolor='none')\n",
    "    # rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth=1, edgecolor='r', facecolor='none')\n",
    "\n",
    "    # ax.add_patch(rect)\n",
    "    # plt.show()\n",
    "\n",
    "    # Break after first batch just for demonstration\n",
    "    if i > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = '/usr/mvl2/ksgh2/Documents/projects/VOT/Datasets/data/CNT_Growth_RK/exp1_gf_clahe'\n",
    "files314_pathes = glob.glob(address +  '/* 314 -*.tif')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n",
      "Current device: NVIDIA GeForce RTX 2080 Ti\n",
      "Feature Extractor is:  aliked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1356it [00:02, 460.18it/s]\n"
     ]
    }
   ],
   "source": [
    "tracker = FeatureTracker(ftExtractor = 'aliked', mx_keypoints = 1024, desired_device = 3)\n",
    "\n",
    "plottingFrames = []\n",
    "gr_exp_1_314 = []\n",
    "\n",
    "\n",
    "for i, imagePath in tqdm(enumerate(files314_pathes)):\n",
    "    # Get the base name of the file\n",
    "    basename = os.path.basename(imagePath)\n",
    "    # Remove 'exp1_gf_clahe/' from the beginning of the name\n",
    "    orginal_name = basename.replace('gf_clh_', '')\n",
    "\n",
    "    if i < 0:\n",
    "        gr_exp_1_314.append((orginal_name, -1))\n",
    "    elif i < 10:\n",
    "        idx = i - 0\n",
    "        desRec = [0, 0, 1024, 450]\n",
    "        isOnline = False\n",
    "        __cpTracks, __kpMvmt, image_cv2, _frmTracks = tracker.process_frame(imagePath, idx, desRec, isOnline)\n",
    "        # First, filter out tracks with less than 3 keypoints to avoid modifying the dictionary during iteration\n",
    "        # __cpTracks = {key: value for key, value in __cpTracks.items() if len(value) >= 3}\n",
    "\n",
    "        # # Create a dictionary that maps track IDs to colors\n",
    "\n",
    "        cvFrame = tracker.update_cvFrame(__cpTracks, image_cv2, _frmTracks, desRec, _isOnline = False)\n",
    "        plottingFrames.append(cvFrame)\n",
    "\n",
    "        nmGrowthrate = 0\n",
    "        if __kpMvmt > 0:\n",
    "            nmGrowthrate = np.log(__kpMvmt * 5) # conversion to nm and log scale 400px = 2um -> 1 px = 2um/400px = 5nm/px\n",
    "        gr_exp_1_314.append((orginal_name, nmGrowthrate))\n",
    "    if i >= 1350:\n",
    "        gr_exp_1_314.append((orginal_name, -1))\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video was successfully saved!\n"
     ]
    }
   ],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Use 'mp4v' for .mp4 format\n",
    "\n",
    "# Use the shape of the first image to get the frame size (width, height)\n",
    "frame_size = (plottingFrames[0].shape[1], plottingFrames[0].shape[0])\n",
    "\n",
    "# Create a VideoWriter object. Adjust FPS to your liking.\n",
    "out = cv2.VideoWriter('output_video.mp4', fourcc, 1.0, frame_size)\n",
    "\n",
    "for i in range(len(plottingFrames)):\n",
    "    # Write the frame to the video\n",
    "    out.write(plottingFrames[i])\n",
    "\n",
    "# Release everything when job is finished\n",
    "out.release()\n",
    "\n",
    "# Optionally, display a message\n",
    "print(\"The video was successfully saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
